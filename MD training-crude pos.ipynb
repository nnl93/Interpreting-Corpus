{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['百丽和平常一样猛地推门进屋的一刻洛枳刚收起日记拿起笔继续计算数理统计学的作业题', '。', ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk \n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "corpus = CategorizedPlaintextCorpusReader(\n",
    "    '/users/nannanliu/Python/SCIPPC/ToRCH2014/individual/general_fiction',\n",
    "    r'(?!\\.).*\\.txt',\n",
    "    cat_pattern=os.path.join(r'(neg|pos)', '.*',),\n",
    "    encoding='utf-8')\n",
    "corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general_fiction_01.txt\n",
      "general_fiction_02.txt\n",
      "general_fiction_03.txt\n",
      "general_fiction_04.txt\n",
      "general_fiction_05.txt\n",
      "general_fiction_06.txt\n",
      "general_fiction_07.txt\n",
      "general_fiction_08.txt\n",
      "general_fiction_09.txt\n",
      "general_fiction_10.txt\n",
      "general_fiction_11.txt\n",
      "general_fiction_12.txt\n",
      "general_fiction_13.txt\n",
      "general_fiction_14.txt\n",
      "general_fiction_15.txt\n",
      "general_fiction_16.txt\n",
      "general_fiction_17.txt\n",
      "general_fiction_18.txt\n",
      "general_fiction_19.txt\n",
      "general_fiction_20.txt\n",
      "general_fiction_21.txt\n",
      "general_fiction_22.txt\n",
      "general_fiction_23.txt\n",
      "general_fiction_24.txt\n",
      "general_fiction_25.txt\n",
      "general_fiction_26.txt\n",
      "general_fiction_27.txt\n",
      "general_fiction_28.txt\n",
      "general_fiction_29.txt\n",
      "general_fiction_30.txt\n",
      "general_fiction_31.txt\n",
      "general_fiction_32.txt\n"
     ]
    }
   ],
   "source": [
    "#check if fileids are correct\n",
    "files=corpus.fileids()\n",
    "for f in files: \n",
    "    print (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynlpir\n",
    "pynlpir.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora=[]\n",
    "for file in files: \n",
    "    sub_corpora=corpus.raw(file)\n",
    "    corpora.append(sub_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_files=[]\n",
    "for sub_corpora in corpora: \n",
    "    tagged_file=pynlpir.segment(sub_corpora, pos_tagging=True, pos_names='parent')\n",
    "    tagged_files.append(tagged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_list=[]\n",
    "for file in tagged_files: \n",
    "    none_list.append([s for s in file if None in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print (none_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(32): \n",
    "    for n, i in enumerate(tagged_files[j]):\n",
    "        if i == ('\\r新华社', None):\n",
    "            tagged_files[j][n] = ('\\r新华社', 'noun-proper')\n",
    "        if i == ('新华社', None):\n",
    "            tagged_files[j][n] = ('新华社', 'noun-proper')\n",
    "        if i == ('\\r新华网', None):\n",
    "            tagged_files[j][n] = ('\\r新华网', 'noun-proper')\n",
    "        if i == ('新华网', None):\n",
    "            tagged_files[j][n] = ('新华网', 'noun-proper')\n",
    "        if i == ('中新网', None):\n",
    "            tagged_files[j][n] = ('中新网', 'noun-proper')\n",
    "        if i == ('人民网', None):\n",
    "            tagged_files[j][n] = ('人民网', 'noun-proper')\n",
    "        if i == ('\\r中国青年网', None):\n",
    "            tagged_files[j][n] = ('\\r中国青年网', 'noun-proper')\n",
    "        if i == ('中评社', None):\n",
    "            tagged_files[j][n] = ('中评社', 'noun-proper')\n",
    "        if i == ('\\r中国日报网', None):\n",
    "            tagged_files[j][n] = ('\\r中国日报网', 'noun-proper')\n",
    "        if i == ('南华早报', None):\n",
    "            tagged_files[j][n] = ('南华早报', 'noun-proper')\n",
    "        if i == ('\\r国际在线', None):\n",
    "            tagged_files[j][n] = ('\\r国际在线', 'noun-proper')\n",
    "        if i == ('新华社', None):\n",
    "            tagged_files[j][n] = ('新华社', 'noun-proper')\n",
    "        if i == ('派', None): \n",
    "            tagged_files[j][n] = ('派', 'noun-verb')\n",
    "        if i == ('网民', None): \n",
    "            tagged_files[j][n] = ('网民', 'noun')\n",
    "        if i == ('屌丝', None):\n",
    "            tagged_files[j][n] = ('屌丝', 'noun')\n",
    "        if i == ('\\r屌丝', None):\n",
    "            tagged_files[j][n] = ('\\r屌丝', 'noun')\n",
    "        if i == ('富帅', None):\n",
    "            tagged_files[j][n] = ('富帅', 'noun')\n",
    "        if i == ('解构', None): \n",
    "            tagged_files[j][n] = ('解构', 'noun-verb')\n",
    "        if i == ('身份卑微', None): \n",
    "            tagged_files[j][n] = ('身份卑微', 'adjective')\n",
    "        if i == ('\\r南方日报', None): \n",
    "            tagged_files[j][n] = ('\\r南方日报', 'noun')\n",
    "        if i == ('法新社', None):\n",
    "            tagged_files[j][n] = ('法新社', 'noun-proper')\n",
    "        if i == ('美联社', None):\n",
    "            tagged_files[j][n] = ('美联社', 'noun-proper')\n",
    "        if i == ('路透社', None):\n",
    "            tagged_files[j][n] = ('路透社', 'noun-proper')\n",
    "        if i == ('环球时报', None):\n",
    "            tagged_files[j][n] = ('环球时报', 'noun-proper')\n",
    "        if i == ('飞机', None):\n",
    "            tagged_files[j][n] = ('飞机', 'noun')\n",
    "        if i == ('甲', None): \n",
    "            tagged_files[j][n] = ('甲', 'numeral')\n",
    "        if i == ('乙', None): \n",
    "            tagged_files[j][n] = ('乙', 'numeral')\n",
    "        if i == ('丙', None): \n",
    "            tagged_files[j][n] = ('丙', 'numeral')\n",
    "        if i == ('丁', None): \n",
    "            tagged_files[j][n] = ('丁', 'numeral')\n",
    "        if i == ('辰', None): \n",
    "            tagged_files[j][n] = ('辰', 'numeral')\n",
    "        if i == ('癸', None): \n",
    "            tagged_files[j][n] = ('癸', 'numeral')  \n",
    "        if i == ('戊', None): \n",
    "            tagged_files[j][n] = ('戊', 'numeral')\n",
    "        if i == ('巳', None): \n",
    "            tagged_files[j][n] = ('巳', 'numeral')\n",
    "        if i == ('\\u3000', None): \n",
    "            tagged_files[j][n] = ('\\u3000', 'None')\n",
    "        if i == ('贴吧', None): \n",
    "            tagged_files[j][n] = ('贴吧', 'noun')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def verb_de(text): \n",
    "    verb_list=list(map(list, zip([item for item in text if re.match(r'\\bverb\\b', item[1])], text[1:])))\n",
    "    flat_list = [item for sublist in verb_list for item in sublist]\n",
    "    return round ((flat_list.count(('的', 'particle'))/len(text))*1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "df = pd.read_csv(\"/users/nannanliu/Research/MD/stats/individual/general_fiction.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_de_result=[]\n",
    "for file in tagged_files: \n",
    "    verb_de_result.append(verb_de(file))\n",
    "\n",
    "df['verb_de'] = pd.Series(verb_de_result)\n",
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/general_fiction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NOMZ'] = df['NOMZ'] + df['verb_de']\n",
    "del df['verb_de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/users/nannanliu/Research/MD/stats/individual/general_fiction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
