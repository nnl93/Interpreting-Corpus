{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'morning', 'ladies', 'and', 'gentlemen', ',', ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk \n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "corpus = CategorizedPlaintextCorpusReader(\n",
    "    '/users/nannanliu/Python/SCIPPC/interpreted_English/raw',\n",
    "    r'(?!\\.).*\\.txt',\n",
    "    cat_pattern=os.path.join(r'(neg|pos)', '.*',),\n",
    "    encoding='utf-8')\n",
    "corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003_interpreted_English.txt\n",
      "2004_interpreted_English.txt\n",
      "2005_interpreted_English.txt\n",
      "2006_interpreted_English.txt\n",
      "2007_interpreted_English.txt\n",
      "2013_interpreted_English.txt\n",
      "2014_interpreted_English.txt\n",
      "2015_interpreted_English.txt\n",
      "2016_interpreted_English.txt\n",
      "2017_interpreted_English.txt\n",
      "IE_total.txt\n"
     ]
    }
   ],
   "source": [
    "#check if fileids are correct\n",
    "files=corpus.fileids()\n",
    "for f in files: \n",
    "    print (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47711"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make total text \n",
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/raw/2017_interpreted_English.txt') as f:\n",
    "    lines = f.read()\n",
    "output=open('/users/nannanliu/Python/SCIPPC/interpreted_English/raw/IE_total.txt', 'a')\n",
    "output.write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2003=corpus.raw('2003_interpreted_English.txt')\n",
    "text_2004=corpus.raw('2004_interpreted_English.txt')\n",
    "text_2005=corpus.raw('2005_interpreted_English.txt')\n",
    "text_2006=corpus.raw('2006_interpreted_English.txt')\n",
    "text_2007=corpus.raw('2007_interpreted_English.txt')\n",
    "text_2013=corpus.raw('2013_interpreted_English.txt')\n",
    "text_2014=corpus.raw('2014_interpreted_English.txt')\n",
    "text_2015=corpus.raw('2015_interpreted_English.txt')\n",
    "text_2016=corpus.raw('2016_interpreted_English.txt')\n",
    "text_2017=corpus.raw('2017_interpreted_English.txt')\n",
    "text_total=corpus.raw('IE_total.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw texts\n",
    "tokens_2003=nltk.word_tokenize(text_2003)\n",
    "tokens_2004=nltk.word_tokenize(text_2004)\n",
    "tokens_2005=nltk.word_tokenize(text_2005)\n",
    "tokens_2006=nltk.word_tokenize(text_2006)\n",
    "tokens_2007=nltk.word_tokenize(text_2007)\n",
    "tokens_2013=nltk.word_tokenize(text_2013)\n",
    "tokens_2014=nltk.word_tokenize(text_2014)\n",
    "tokens_2015=nltk.word_tokenize(text_2015)\n",
    "tokens_2016=nltk.word_tokenize(text_2016)\n",
    "tokens_2017=nltk.word_tokenize(text_2017)\n",
    "tokens_total=nltk.word_tokenize(text_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74022"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2003=text_2003.lower()\n",
    "temp_2004=text_2004.lower()\n",
    "temp_2005=text_2005.lower()\n",
    "temp_2006=text_2006.lower()\n",
    "temp_2007=text_2007.lower()\n",
    "temp_2013=text_2013.lower()\n",
    "temp_2014=text_2014.lower()\n",
    "temp_2015=text_2015.lower()\n",
    "temp_2016=text_2016.lower()\n",
    "temp_2017=text_2017.lower()\n",
    "temp_total=text_total.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2013=temp_2013.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2014=temp_2014.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2015=temp_2015.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2016=temp_2016.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2017=temp_2017.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2003=temp_2003.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2004=temp_2004.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2005=temp_2005.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2006=temp_2006.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_2007=temp_2007.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")\n",
    "temp_total=temp_total.replace(\"'\", \" \").replace(\"’\", \" \").replace(\"we.\", \"we .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_2003=nltk.word_tokenize(temp_2003)\n",
    "text_2003=nltk.Text(tokens_2003)\n",
    "\n",
    "tokens_2004=nltk.word_tokenize(temp_2004)\n",
    "text_2004=nltk.Text(tokens_2004)\n",
    "\n",
    "tokens_2005=nltk.word_tokenize(temp_2005)\n",
    "text_2005=nltk.Text(tokens_2005)\n",
    "\n",
    "tokens_2006=nltk.word_tokenize(temp_2006)\n",
    "text_2006=nltk.Text(tokens_2006)\n",
    "\n",
    "tokens_2007=nltk.word_tokenize(temp_2007)\n",
    "text_2007=nltk.Text(tokens_2007)\n",
    "\n",
    "tokens_2013=nltk.word_tokenize(temp_2013)\n",
    "text_2013=nltk.Text(tokens_2013)\n",
    "\n",
    "tokens_2014=nltk.word_tokenize(temp_2014)\n",
    "text_2014=nltk.Text(tokens_2014)\n",
    "\n",
    "tokens_2015=nltk.word_tokenize(temp_2015)\n",
    "text_2015=nltk.Text(tokens_2015)\n",
    "\n",
    "tokens_2016=nltk.word_tokenize(temp_2016)\n",
    "text_2016=nltk.Text(tokens_2016)\n",
    "\n",
    "tokens_2017=nltk.word_tokenize(temp_2017)\n",
    "text_2017=nltk.Text(tokens_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_total=nltk.word_tokenize(temp_total)\n",
    "text_total=nltk.Text(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text  \n",
    "\n",
    "stopWords=set(stopwords.words('english'))\n",
    "new_stopwords = set(stopwords.words('english')) - {'we', 'our', 'us', 'would', 'will', 'have', 'can', 'should', 'ours', 'don'}\n",
    "\n",
    "words_2003=text_2003\n",
    "wordsFiltered_2003=[]\n",
    "\n",
    "for w in words_2003: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2003.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2004=text_2004\n",
    "wordsFiltered_2004=[]\n",
    "\n",
    "for w in words_2004: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2004.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2005=text_2005\n",
    "wordsFiltered_2005=[]\n",
    "\n",
    "for w in words_2005: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2005.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2006=text_2006\n",
    "wordsFiltered_2006=[]\n",
    "\n",
    "for w in words_2006: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2006.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2007=text_2007\n",
    "wordsFiltered_2007=[]\n",
    "\n",
    "for w in words_2007: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2007.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2013=text_2013\n",
    "wordsFiltered_2013=[]\n",
    "\n",
    "for w in words_2013: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2013.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2014=text_2014\n",
    "wordsFiltered_2014=[]\n",
    "\n",
    "for w in words_2014: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2014.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2015=text_2015\n",
    "wordsFiltered_2015=[]\n",
    "\n",
    "for w in words_2015: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2015.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2016=text_2016\n",
    "wordsFiltered_2016=[]\n",
    "\n",
    "for w in words_2016: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2016.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_2017=text_2017\n",
    "wordsFiltered_2017=[]\n",
    "\n",
    "for w in words_2017: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_2017.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_total=text_total\n",
    "wordsFiltered_total=[]\n",
    "\n",
    "for w in words_total: \n",
    "    if w not in new_stopwords: \n",
    "        wordsFiltered_total.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "porter=nltk.PorterStemmer()\n",
    "stemmed_2003=[porter.stem(t) for t in wordsFiltered_2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2004=[porter.stem(t) for t in wordsFiltered_2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2005=[porter.stem(t) for t in wordsFiltered_2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2006=[porter.stem(t) for t in wordsFiltered_2006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2007=[porter.stem(t) for t in wordsFiltered_2007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2013=[porter.stem(t) for t in wordsFiltered_2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2014=[porter.stem(t) for t in wordsFiltered_2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2015=[porter.stem(t) for t in wordsFiltered_2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2016=[porter.stem(t) for t in wordsFiltered_2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2017=[porter.stem(t) for t in wordsFiltered_2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_total=[porter.stem(t) for t in wordsFiltered_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=nltk.WordNetLemmatizer()\n",
    "lemma_2003=[wnl.lemmatize(t) for t in stemmed_2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2004=[wnl.lemmatize(t) for t in stemmed_2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2005=[wnl.lemmatize(t) for t in stemmed_2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2006=[wnl.lemmatize(t) for t in stemmed_2006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2007=[wnl.lemmatize(t) for t in stemmed_2007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2013=[wnl.lemmatize(t) for t in stemmed_2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2014=[wnl.lemmatize(t) for t in stemmed_2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2015=[wnl.lemmatize(t) for t in stemmed_2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2016=[wnl.lemmatize(t) for t in stemmed_2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2017=[wnl.lemmatize(t) for t in stemmed_2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_total=[wnl.lemmatize(t) for t in stemmed_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2003_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2003:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2004_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2004:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2005_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2005:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2006_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2006:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2007_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2007:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2013_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2013:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2014_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2014:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2015_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2015:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2016_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2016:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/2017_IE_processed.txt', 'w') as f:\n",
    "    for item in lemma_2017:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/nannanliu/Python/SCIPPC/interpreted_English/processed/IE_total_processed.txt', 'w') as f:\n",
    "    for item in lemma_total:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
